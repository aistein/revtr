#!/usr/local/bin/python3

# stage-1-split.py
# -------------------
# usage:
# - ./stage-1-split.py
#
# function:
# - create the following file hierarchy for test and training data
# --train-----vp_measurements (4/5 of input data)
#                    |
#                <vp_name>.csv
#                dnet, dst, [hops path]
# --test------vp_measurements (1/5 of input data)
#                    |
#                <vp_name>.csv
#                dnet, dst, [hops path]
#
# note:
# - the input JSON file is expected to have the following format
#    {
#       <destination-network>:
#           (collection of destination IP's)
#    }
# - "training" split will contain 4/5ths of 'unique dest-IPs' per destination-net
# - "test" split will contain 1/5th of 'unique dest-IPs' per destination-net
# - In other words, "training" and "test" are guaranteed NOT to have the same
#   destination IPs, but will still properly split measurements from the same dnet. 

import sys
import os
import yaml
import json
import time
import csv
#import threading
import multiprocessing as mp
from collections import defaultdict

import pyasn
from scripts.lookup_dnet import dnets_of

#==========================================================
# Configuration Parsing (YAML)
#==========================================================

try:
    configfile = open("./pipeline/configurations/stage-1-split.yml", 'r')
    configurations = yaml.load(configfile)
    configfile.close()
except (FileNotFoundError,IndexError):
    exit('error: stage-1-split.yml either not found or incorrect.')


# flag indicating if the measurement files need to be downloaded or if they are stored locally
download = configurations['download']

# directory where VP files will be stored
vpdir = configurations['vpdir']

# directory where all output data should be stored
datadir = configurations['datadir']

# bgpdump ipasn file
ipasnfile = configurations['bgpdump']

# the number of CPUs available on the running machine
numcpus = configurations['numcpus']

# sources / outputs
traindir = datadir + "/train/vp_measurements/all"
testdir = datadir + "/test/vp_measurements/all"

#==========================================================
# Core Data Splitting Functions
#==========================================================

# ddict
# dictionary that will hold decision to place in train/test for all destinations
# items look like {... , dest-ip: ("train"/"test", DnetEntry)}
ddict = defaultdict(tuple)

# writer
# Receive transformed data from worker processes through the specified input queue.
# Each output file per VP has only a single writer process.
def writer(q, lock, target, vpfile):
    with lock:
        print("starting writer-{}-{} for VP {}".format(target, os.getpid(), vpfile))

    f = None
    if target == 'train':
        f = open(os.path.join(traindir, vpfile), 'w')
    else:
        f = open(os.path.join(testdir, vpfile), 'w')

    f.write("asn,bgp,s24,dest,hop1,hop2,hop3,hop4,hop5,hop6,hop7,hop8,hop9\n")
    f.flush()

    while True:
        m = q.get()
        if m == 'kill':
            break
        #with lock:
        #    print("writer-{}-{} writing line: {}".format(target, os.getpid(), m)) 
        f.write(m)
        f.flush()

    f.close()
    with lock:
        print("exiting writer-{}-{} for VP {}".format(target, os.getpid(), vpfile))
    return

# worker
# Given 'vpfile'.csv (in RAM), create two mappings "train" and "test".
# The split is based on tags assigned to each destination IP address, to be found in ddict
def worker(data, left, right, mp_ddict, train_queue, test_queue, lock):
    with lock:
        print("starting worker {}".format(os.getpid()))
        #print("data is sized {}".format(len(data)))
        #print("chunk is sized {}".format(right-left))
        #print("ddict is sized {}".format(len(mp_ddict)))
    for i in range (left, right):
        probe = data[i].split(',')
        dest, hops = probe[0], probe[1:]
        if dest in mp_ddict.keys():
            target, dentry = mp_ddict[dest]
            if target == "train":
                train_queue.put("{},{},{},{},{}".format(
                    dentry.asn, dentry.bgp, dentry.s24, dest, ','.join(hops)))
            if target == "test":
                test_queue.put("{},{},{},{},{}".format(
                    dentry.asn, dentry.bgp, dentry.s24, dest, ','.join(hops)))

    with lock:
        print("worker {} exiting...".format(os.getpid()))

    return

#==========================================================
# File/Dir Processing Functions
#==========================================================

# SetupDirs ...
# Ensures that the directories we need are correctly set up
def SetupDirs():
    if not os.path.exists(datadir):
        os.makedirs(datadir)
    if not os.path.exists(traindir):
        os.makedirs(traindir)
    if not os.path.exists(testdir):
        os.makedirs(testdir)

#==========================================================
# Main
#==========================================================

def main():
    start = time.time()
    
    mapfile = datadir + "/mappings/dests_by_prefix.json"
    ipasn = pyasn.pyasn(ipasnfile)
    # Split the annotated, filtered measurement data into two sets
    num_prefixes = 0
    with open(mapfile, 'r') as jsonfile:
        json_data = json.load(jsonfile)
        for prefix, dests in json_data.items():
            print("tagging measurements for prefix {}\n".format(prefix))
            if len(dests) > 1:
                index = 0
                num_prefixes = num_prefixes + 1
                for dest in dests:
                    dentry = dnets_of(dest, ipasn)
                    # ~1/5th of data goes in the testing set
                    if index % 5 == 0:
                        ddict[dest] = ("test", dentry)
                    # ~4/5ths of data goes in the training set
                    else:
                        ddict[dest] = ("train", dentry)
                    # increment the index
                    index = index + 1

    print("Input contained {} unique, viable destination networks.\n".format(num_prefixes))
    print("Identified {} unique destination IPs to split.\n".format(len(ddict.keys())))

    # 3 dnet definitions
    # - The only actual grouping should be by BGP-prefix
    # - Want 2 more copies of the BGP-grouping that assign ASN and S24

    # setup the needed directory structure
    SetupDirs()

    # for each VP, create trianing and test CSV files
    # -- putting this in a separate loop avoids opening each vpfile several times
    # -- each VP will be processed in parallel with 'threading'
    
    # use Manager so that processes can pass stuff to eachother
    manager = mp.Manager()
    train_queue, test_queue = manager.Queue(), manager.Queue()
    lock = manager.Lock()

    # Process each VP using producer/consumer pattern on a pool of workers
    for vpfile in os.listdir(vpdir):
        # Read the entire file into RAM
        print("Reading file {} into memory...".format(vpfile))
        with open(os.path.join(vpdir, vpfile), 'r') as csvfile:
            data = manager.list(csvfile.readlines())

        print("How much Data? {}".format(len(data)))

        # calculate pool configurations
        num_processes = (2 * numcpus) // 3
        num_consumers = 2
        num_producers = num_processes - num_consumers
        chunksize, leftover = len(data) // num_producers, len(data) % num_producers
        left, to_deploy = leftover, num_producers
         
        pool = mp.Pool(processes=num_processes)

        producers = []
        consumers = []

        # create the producers
        print("Creating Producers...")
        if leftover:
            producer = mp.Process(target=worker,
                args=(data, 0, left, ddict, train_queue, test_queue, lock))
            producers.append(producer)
            to_deploy -= 1
        for i in range(to_deploy):
            producer = mp.Process(target=worker, 
                    args=(data, left, left+chunksize, ddict, train_queue, test_queue, lock))
            producers.append(producer)
            left += chunksize

        # create the two consumers
        print("Creating Consumers...")
        for target in ['train', 'test']:
            queue = train_queue if target == 'train' else test_queue
            consumer = mp.Process(target=writer, args=(queue, lock, target, vpfile))
            consumers.append(consumer)

        # start producers
        print("Starting Producers...")
        for producer in producers:
            producer.start()

        # start consumers
        with lock:
            print("Starting Consumers...")
        for consumer in consumers:
            consumer.start()
                
        # wait for all producers to finish
        with lock:
            print("Waiting for Producers to complete...")
        for p in producers:
            p.join()
            with lock:
                print("producer completed.")

        with lock:
            print("{} remaining to process for train.".format(train_queue.qsize()))
            print("{} remaining to process for test.".format(test_queue.qsize()))

        train_queue.put('kill')
        test_queue.put('kill')
        for c in consumers:
            c.join()
            with lock:
                print("consumer completed.")

        # close the thread pool
        pool.close()

    end = time.time()
    print("Done. time elapsed: {}".format(end-start)) 

if __name__ == "__main__":
    main()
